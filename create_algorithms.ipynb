{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hosw_P5K78b9",
        "outputId": "2d8c1aab-0a38-498c-a9a5-663cccea4817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting implicit\n",
            "  Downloading implicit-0.7.2-cp310-cp310-manylinux2014_x86_64.whl (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nearpy\n",
            "  Downloading NearPy-1.0.0-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lshashpy3\n",
            "  Downloading lshashpy3-0.0.9.tar.gz (9.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from implicit) (4.66.4)\n",
            "Collecting bitarray (from nearpy)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from nearpy) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from lshashpy3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: annoy, lshashpy3\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.3-cp310-cp310-linux_x86_64.whl size=552447 sha256=0de55fa44694d34e64e6c3d39dac906a2388e65271f967a530d8e0e409794e8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/8a/da/f714bcf46c5efdcfcac0559e63370c21abe961c48e3992465a\n",
            "  Building wheel for lshashpy3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lshashpy3: filename=lshashpy3-0.0.9-py3-none-any.whl size=8875 sha256=01a18539ee97b411304cfc961a79c19463e8d426f8bc3d439ee496c6158a20e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/f0/83/666c2757c9fbd2d87814f40a5fc3444d3a81cf8f6a912e4321\n",
            "Successfully built annoy lshashpy3\n",
            "Installing collected packages: bitarray, annoy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lshashpy3, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nearpy, implicit, nvidia-cusolver-cu12\n",
            "Successfully installed annoy-1.17.3 bitarray-2.9.2 implicit-0.7.2 lshashpy3-0.0.9 nearpy-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy pandas scikit-learn annoy implicit nearpy lshashpy3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmXA0i1X8bJR",
        "outputId": "bdfac770-52cd-4f8f-e648-a8abdbcdc4a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from annoy import AnnoyIndex\n",
        "import gc\n",
        "from scipy.sparse import coo_matrix\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from nearpy import Engine\n",
        "from nearpy.hashes import RandomBinaryProjections\n",
        "import json\n",
        "\n",
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SM4ZuMP7DIpP"
      },
      "outputs": [],
      "source": [
        "# Rutas a los archivos en tu Google Drive\n",
        "data_folder = '/content/drive/MyDrive/RecSys/'\n",
        "\n",
        "# Leer data/train_data.csv\n",
        "train_data = pd.read_csv(data_folder + 'data/train_data.csv')\n",
        "\n",
        "# Leer el pickle embeddings_train.pkl\n",
        "with open(data_folder + 'data/embeddings_train.pkl', 'rb') as f:\n",
        "    embeddings_dict_train = pickle.load(f)\n",
        "\n",
        "# Convertir diccionario de embeddings a una matriz\n",
        "track_ids = list(embeddings_dict_train.keys())\n",
        "embeddings_matrix = np.array([embeddings_dict_train[tid] for tid in track_ids])\n",
        "\n",
        "# Realizar PCA con 104 componentes (capturando el 95% de la varianza)\n",
        "pca = PCA(n_components=104)\n",
        "embeddings_matrix_reduced = pca.fit_transform(embeddings_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD6a7HCKF8SE"
      },
      "outputs": [],
      "source": [
        "# Parámetros\n",
        "hidden_dim = 128\n",
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Definición del Autoencoder\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "# Convertir embeddings_matrix_reduced a tensor\n",
        "embeddings_tensor = torch.tensor(embeddings_matrix_reduced, dtype=torch.float32)\n",
        "\n",
        "# Crear DataLoader\n",
        "dataset = TensorDataset(embeddings_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Inicializar el modelo, criterio y optimizador\n",
        "model = Autoencoder(input_dim=104, hidden_dim=hidden_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs3giDshGChz"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento del autoencoder\n",
        "for epoch in range(num_epochs):\n",
        "    for data in dataloader:\n",
        "        inputs = data[0]\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "torch.save(model.state_dict(), data_folder + 'autoencoder.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGNkMvUyFo_s",
        "outputId": "92a924a0-4f5e-427f-e7f4-6ab55d111417"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Autoencoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=104, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=104, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cargar el modelo entrenado\n",
        "model = Autoencoder(input_dim=104, hidden_dim=hidden_dim)\n",
        "model.load_state_dict(torch.load(data_folder + 'autoencoder.pth'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrYnbrHGFt06"
      },
      "outputs": [],
      "source": [
        "# Construir el índice Annoy para los embeddings de canciones\n",
        "def build_annoy_index(embeddings_tensor, batch_size=10000):\n",
        "    index = AnnoyIndex(hidden_dim, 'angular')\n",
        "    num_batches = len(embeddings_tensor) // batch_size + 1\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        start = batch * batch_size\n",
        "        end = min((batch + 1) * batch_size, len(embeddings_tensor))\n",
        "        batch_embeddings = embeddings_tensor[start:end]\n",
        "\n",
        "        for i, embedding in enumerate(batch_embeddings):\n",
        "            with torch.no_grad():\n",
        "                compressed_embedding = model.encoder(embedding.unsqueeze(0)).squeeze()\n",
        "            index.add_item(start + i, compressed_embedding.numpy())\n",
        "\n",
        "        # Liberar memoria de los embeddings y el modelo en cada lote\n",
        "        del batch_embeddings\n",
        "        gc.collect()\n",
        "\n",
        "        print(f'Processed batch {batch + 1}/{num_batches}')\n",
        "\n",
        "    index.build(10)  # Ajustar el número de árboles según sea necesario\n",
        "    return index\n",
        "\n",
        "# Construir y guardar el índice Annoy\n",
        "index = build_annoy_index(embeddings_tensor)\n",
        "index.save(data_folder + 'annoy_index.ann')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRH_1-3TNTVG",
        "outputId": "5a701f7c-92f3-4f32-f2f6-524128dd0187"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Abrir index de data_folder + 'annoy_index.ann'\n",
        "index = AnnoyIndex(hidden_dim, 'angular')\n",
        "index.load(data_folder + 'annoy_index.ann')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlHHevvblVfP"
      },
      "outputs": [],
      "source": [
        "track_id_to_index = {track_id: i for i, track_id in enumerate(track_ids)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EZCVBqGY9jG",
        "outputId": "e4f23d5e-0787-41c1-8826-16e21430815c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recomendaciones guardadas en recommendations_ann.json\n"
          ]
        }
      ],
      "source": [
        "# Función para obtener embeddings de playlists\n",
        "def get_playlist_embeddings(playlist_track_ids):\n",
        "    valid_track_ids = [tid for tid in playlist_track_ids if tid in track_id_to_index]\n",
        "    valid_indices = [track_id_to_index[tid] for tid in valid_track_ids]\n",
        "    track_embeddings = embeddings_matrix_reduced[valid_indices]  # Usar embeddings reducidos\n",
        "    track_embeddings_tensor = torch.tensor(track_embeddings, dtype=torch.float32)\n",
        "\n",
        "    # Calcular el embedding de la playlist\n",
        "    with torch.no_grad():\n",
        "        playlist_embedding = model.encoder(track_embeddings_tensor)\n",
        "\n",
        "    # Asegurar que el embedding tenga la dimensión correcta\n",
        "    playlist_embedding = playlist_embedding.mean(dim=0)\n",
        "\n",
        "    return playlist_embedding.numpy()\n",
        "\n",
        "# Función para recomendar canciones a una playlist\n",
        "def recommend_songs(playlist_id, top_n=10):\n",
        "    playlist_track_ids = train_data[train_data['pid'] == playlist_id]['tid'].values\n",
        "    playlist_embedding = get_playlist_embeddings(playlist_track_ids)\n",
        "    similar_items = index.get_nns_by_vector(playlist_embedding, top_n)\n",
        "    return [int(track_ids[i]) for i in similar_items]  # Convertir a int para asegurar serialización\n",
        "\n",
        "# Diccionario para almacenar las recomendaciones\n",
        "recommendations_dict = {}\n",
        "\n",
        "# Iterar sobre todos los pid en train_data\n",
        "for playlist_id in train_data['pid'].unique():\n",
        "    try:\n",
        "        recommendations_dict[str(playlist_id)] = {\n",
        "            'top_10': recommend_songs(playlist_id, top_n=10),\n",
        "            'top_20': recommend_songs(playlist_id, top_n=20)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar la playlist {playlist_id}: {e}\")\n",
        "\n",
        "# Convertir a tipos nativos de Python para serialización JSON\n",
        "recommendations_dict = {k: v if isinstance(v, (int, list, dict)) else v.item() for k, v in recommendations_dict.items()}\n",
        "\n",
        "# Guardar las recomendaciones en un archivo JSON\n",
        "with open(data_folder + 'data/recommendations_ann.json', 'w') as f:\n",
        "    json.dump(recommendations_dict, f)\n",
        "\n",
        "print(\"Recomendaciones guardadas en recommendations_ann.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpKO4clt5PhJ"
      },
      "source": [
        "ALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIyb6-PiKr12"
      },
      "outputs": [],
      "source": [
        "# Crear una lista de listas donde cada sublista contiene los track ids de una playlist\n",
        "playlist_tracks = train_data.groupby('pid')['tid'].apply(list).tolist()\n",
        "\n",
        "# Crear una matriz de interacción (playlists x tracks)\n",
        "mlb = MultiLabelBinarizer()\n",
        "interaction_matrix = mlb.fit_transform(playlist_tracks)\n",
        "interaction_matrix_sparse = coo_matrix(interaction_matrix)\n",
        "\n",
        "# Obtener los índices de las playlists y las canciones\n",
        "playlist_ids = train_data['pid'].unique()\n",
        "track_ids = mlb.classes_\n",
        "\n",
        "# Crear un diccionario para mapear track ids a índices\n",
        "track_id_to_idx = {track_id: idx for idx, track_id in enumerate(track_ids)}\n",
        "\n",
        "# Crear un diccionario para mapear pid a índices\n",
        "playlist_id_to_idx = {pid: idx for idx, pid in enumerate(playlist_ids)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gHWcLSqK8Qq"
      },
      "outputs": [],
      "source": [
        "# Configurar y entrenar el modelo ALS\n",
        "model = AlternatingLeastSquares(factors=128, regularization=0.1, iterations=50)\n",
        "model.fit(interaction_matrix_sparse.T)  # Transponemos la matriz para obtener factores de canciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbt6TFYg4MtR"
      },
      "outputs": [],
      "source": [
        "# Guardar el modelo als\n",
        "with open(data_folder + 'als_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4GKJ9y54WtD"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo als\n",
        "with open(data_folder + 'als_model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgk6XaNtK6cJ",
        "outputId": "4f2c2dec-efe2-4715-96a7-86460466ef83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recomendaciones guardadas en recommendations_als.json\n"
          ]
        }
      ],
      "source": [
        "# Función para recomendar canciones a una playlist utilizando ALS\n",
        "def recommend_songs_als(playlist_id, top_n=10):\n",
        "    # Obtener el índice de la playlist\n",
        "    playlist_idx = playlist_id_to_idx.get(playlist_id)\n",
        "\n",
        "    if playlist_idx is None:\n",
        "        print(f'Playlist con id {playlist_id} no encontrada.')\n",
        "        return []\n",
        "\n",
        "    # Obtener las recomendaciones utilizando el modelo ALS\n",
        "    playlist_factors = model.user_factors[playlist_idx]\n",
        "    scores = np.dot(model.item_factors, playlist_factors)\n",
        "    top_song_indices = np.argsort(-scores)[:top_n]\n",
        "    top_song_ids = [track_ids[idx] for idx in top_song_indices]\n",
        "\n",
        "    return top_song_ids\n",
        "\n",
        "# Diccionario para almacenar las recomendaciones\n",
        "recommendations_dict = {}\n",
        "\n",
        "# Iterar sobre todos los pid en train_data\n",
        "for playlist_id in train_data['pid'].unique():\n",
        "    recommendations_dict[str(playlist_id)] = {\n",
        "        'top_10': [int(song_id) for song_id in recommend_songs_als(playlist_id, top_n=10)],\n",
        "        'top_20': [int(song_id) for song_id in recommend_songs_als(playlist_id, top_n=20)]\n",
        "    }\n",
        "\n",
        "# Guardar las recomendaciones en un archivo JSON\n",
        "with open(data_folder + 'data/recommendations_als.json', 'w') as f:\n",
        "    json.dump(recommendations_dict, f)\n",
        "\n",
        "print(\"Recomendaciones guardadas en recommendations_als.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXgPPuBo5T0z"
      },
      "source": [
        "LSH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xF-sF5Ko8Yk"
      },
      "outputs": [],
      "source": [
        "# Crear una lista de listas donde cada sublista contiene los track ids de una playlist\n",
        "playlist_tracks = train_data.groupby('pid')['tid'].apply(list).tolist()\n",
        "\n",
        "# Crear un diccionario para mapear track_id a índice en embeddings_matrix_reduced\n",
        "track_ids = train_data['tid'].unique()\n",
        "track_id_to_index = {track_id: i for i, track_id in enumerate(track_ids)}\n",
        "\n",
        "# Configurar el motor de LSH con hash binario aleatorio\n",
        "dimension = embeddings_matrix_reduced.shape[1]  # Dimensión de los embeddings reducidos\n",
        "num_hashes = 10  # Número de funciones hash a utilizar\n",
        "lsh_engine = LSHash(num_hashes, dimension)\n",
        "\n",
        "# Indexar los embeddings reducidos con LSH\n",
        "for track_id, embedding in zip(track_ids, embeddings_matrix_reduced):\n",
        "    lsh_engine.index(embedding, extra_data=f'track_{track_id_to_index[track_id]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvYVsnHmpC9K"
      },
      "outputs": [],
      "source": [
        "# Función para buscar vecinos cercanos usando LSH\n",
        "def find_nearest_neighbors(embedding, lsh_engine, top_n=10):\n",
        "    query = lsh_engine.query(embedding, num_results=top_n)\n",
        "    nearest_neighbors = [int(result[0][1].split('_')[1]) for result in query]\n",
        "    return nearest_neighbors\n",
        "\n",
        "# Función para recomendar canciones a una playlist específica usando LSH\n",
        "def recommend_songs_to_playlist(pid, embeddings_matrix_reduced, lsh_engine, top_n=10):\n",
        "    # Obtener los track_ids asociados a la playlist pid\n",
        "    playlist_track_ids = train_data[train_data['pid'] == pid]['tid'].values\n",
        "\n",
        "    # Filtrar los track_ids que están dentro del rango válido de embeddings_matrix_reduced\n",
        "    valid_track_ids = [tid for tid in playlist_track_ids if tid in track_id_to_index]\n",
        "\n",
        "    # Obtener los embeddings reducidos de los tracks de la playlist\n",
        "    playlist_embeddings = [embeddings_matrix_reduced[track_id_to_index[tid]] for tid in valid_track_ids]\n",
        "\n",
        "    # Promedio de embeddings de tracks en la playlist\n",
        "    playlist_embedding = np.mean(playlist_embeddings, axis=0)\n",
        "\n",
        "    # Buscar vecinos cercanos usando LSH\n",
        "    nearest_neighbors = find_nearest_neighbors(playlist_embedding, lsh_engine, top_n=top_n)\n",
        "\n",
        "    return nearest_neighbors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcVnOAljpE8S",
        "outputId": "f99a16b4-f48c-46f9-eba4-54eebdc97edb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recomendaciones guardadas en recommendations_lsh.json\n"
          ]
        }
      ],
      "source": [
        "# Diccionario para almacenar las recomendaciones\n",
        "recommendations_dict = {}\n",
        "\n",
        "# Iterar sobre todos los pid en train_data\n",
        "for playlist_id in train_data['pid'].unique():\n",
        "    recommendations_dict[str(playlist_id)] = {\n",
        "        'top_10': [int(song_id) for song_id in recommend_songs_to_playlist(playlist_id, embeddings_matrix_reduced, lsh_engine, top_n=10)],\n",
        "        'top_20': [int(song_id) for song_id in recommend_songs_to_playlist(playlist_id, embeddings_matrix_reduced, lsh_engine, top_n=20)]\n",
        "    }\n",
        "\n",
        "# Guardar las recomendaciones en un archivo JSON\n",
        "with open(data_folder + 'data/recommendations_lsh.json', 'w') as f:\n",
        "    json.dump(recommendations_dict, f)\n",
        "\n",
        "print(\"Recomendaciones guardadas en recommendations_lsh.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2pZcP0RtI7M"
      },
      "source": [
        "Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tieqnz0vtRZQ",
        "outputId": "4ad1c4f1-3458-423f-8090-d8d86440fae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recomendaciones aleatorias guardadas en recommendations_random.json\n"
          ]
        }
      ],
      "source": [
        "# Obtener todos los track_ids únicos\n",
        "track_ids = train_data['tid'].unique()\n",
        "\n",
        "# Función para recomendar canciones de forma aleatoria\n",
        "def recommend_random_songs(track_ids, top_n=10):\n",
        "    return np.random.choice(track_ids, top_n, replace=False).tolist()\n",
        "\n",
        "# Diccionario para almacenar las recomendaciones aleatorias\n",
        "random_recommendations_dict = {}\n",
        "\n",
        "# Iterar sobre todos los pid en train_data\n",
        "for playlist_id in train_data['pid'].unique():\n",
        "    random_recommendations_dict[str(playlist_id)] = {\n",
        "        'top_10': [int(song_id) for song_id in recommend_random_songs(track_ids, top_n=10)],\n",
        "        'top_20': [int(song_id) for song_id in recommend_random_songs(track_ids, top_n=20)]\n",
        "    }\n",
        "\n",
        "# Guardar las recomendaciones aleatorias en un archivo JSON\n",
        "with open(data_folder + 'data/recommendations_random.json', 'w') as f:\n",
        "    json.dump(random_recommendations_dict, f)\n",
        "\n",
        "print(\"Recomendaciones aleatorias guardadas en recommendations_random.json\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
