{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chile\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo data/df_lyrics_clean.json\n",
    "df_tracks = pd.read_json('data/df_lyrics_clean.json')\n",
    "# Leer el archivo data/playlist.csv\n",
    "df_playlist = pd.read_csv('data/playlist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear los conjuntos de entrenamiento y prueba\n",
    "def create_train_test_sets(df, test_fraction=0.2):\n",
    "    # Listas para almacenar los conjuntos de datos de prueba y entrenamiento\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    # Iterar sobre cada pid único\n",
    "    for pid, group in df.groupby('pid'):\n",
    "        # Seleccionar aleatoriamente el 20% de las canciones del grupo actual\n",
    "        test_indices = group.sample(frac=test_fraction).index\n",
    "        test_set = group.loc[test_indices]\n",
    "        train_set = group.drop(test_indices)\n",
    "        \n",
    "        # Agregar los resultados a las listas\n",
    "        train_list.append(train_set)\n",
    "        test_list.append(test_set)\n",
    "    \n",
    "    # Concatenar todos los grupos de entrenamiento y prueba en dos DataFrames\n",
    "    train_set = pd.concat(train_list).reset_index(drop=True)\n",
    "    test_set = pd.concat(test_list).reset_index(drop=True)\n",
    "    \n",
    "    return train_set, test_set\n",
    "\n",
    "# Crear los conjuntos de entrenamiento y prueba\n",
    "train_data, test_data = create_train_test_sets(df_playlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verifica si hay una GPU disponible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Usa GPU\n",
    "    print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Usa CPU si no hay GPU disponible\n",
    "    print(\"GPU no disponible, usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Verifica si hay una GPU disponible y configúrala como dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Cargar el modelo y tokenizer de DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)  # Mueve el modelo a la GPU si está disponible\n",
    "\n",
    "# Función para obtener embeddings\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length').to(device)  # Mueve los inputs a la GPU\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Mueve los outputs de vuelta a la CPU para convertir a numpy\n",
    "\n",
    "# Filtrar los track ids en el conjunto de entrenamiento\n",
    "train_tids = train_data['tid'].unique()\n",
    "\n",
    "# Crear un diccionario para almacenar los embeddings de entrenamiento\n",
    "embeddings_dict_train = {}\n",
    "\n",
    "# Generar embeddings solo para los tracks en el conjunto de entrenamiento\n",
    "for tid in train_tids:\n",
    "    track_info = df_tracks[df_tracks['tid'] == tid].iloc[0]\n",
    "    content = f\"{track_info['track_name']} {track_info['album_name']} {track_info['artist_name']} {track_info['lyrics']}\"\n",
    "    embeddings_dict_train[tid] = get_embeddings(content)\n",
    "\n",
    "# Ahora los embeddings están almacenados en embeddings_dict_train y se han generado usando la GPU para acelerar el proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los embeddings junto con su track id\n",
    "with open('data/embeddings_train.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_dict_train, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chile\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DE ACA CARGA LOS EMBEDDINGS QUE SUBI AL ONEDRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el pickle embeddings_train.pkl\n",
    "with open('data/embeddings_train.pkl', 'rb') as f:\n",
    "    embeddings_dict_train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el diccionario de embeddings a una matriz\n",
    "track_ids = list(embeddings_dict_train.keys())\n",
    "embeddings_matrix = np.array([embeddings_dict_train[tid] for tid in track_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "embeddings_matrix_reduced = pca.fit_transform(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando procesos de cálculo de similitud...\n",
      "Procesos de cálculo de similitud completados.\n",
      "Cálculo de similitud completado y guardado en disco.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "# Definir la ubicación en el disco D:\\ para los archivos temporales\n",
    "temp_dir_d_drive = 'D:\\\\temp_similarity'\n",
    "\n",
    "def compute_similarity_batch(embeddings_matrix, batch_i, batch_j, file_path):\n",
    "    print(f\"Calculando similitud para lotes {batch_i} y {batch_j}\")  # Mensaje de depuración\n",
    "    similarities = cosine_similarity(embeddings_matrix[batch_i[0]:batch_i[1]],\n",
    "                                     embeddings_matrix[batch_j[0]:batch_j[1]])\n",
    "    save_npz(file_path, csr_matrix(similarities))\n",
    "    print(f\"Similitud guardada en {file_path}\")  # Mensaje de depuración\n",
    "\n",
    "def compute_similarity_parallel_joblib(embeddings_matrix, batch_size=1000):\n",
    "    n_tracks = embeddings_matrix.shape[0]\n",
    "\n",
    "    # Crear el directorio temporal en el disco D:\\\n",
    "    os.makedirs(temp_dir_d_drive, exist_ok=True)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for i in range(0, n_tracks, batch_size):\n",
    "        end_i = min(i + batch_size, n_tracks)\n",
    "\n",
    "        for j in range(i, n_tracks, batch_size):  # Solo calcular la mitad superior\n",
    "            end_j = min(j + batch_size, n_tracks)\n",
    "\n",
    "            # Definir el archivo de salida\n",
    "            file_path = os.path.join(temp_dir_d_drive, f'similarity_{i}_{j}.npz')\n",
    "\n",
    "            # Almacenar la información del lote para el proceso paralelo\n",
    "            batches.append((embeddings_matrix, (i, end_i), (j, end_j), file_path))\n",
    "\n",
    "    print(\"Iniciando procesos de cálculo de similitud...\")  # Mensaje de depuración\n",
    "\n",
    "    # Configurar y ejecutar los procesos en paralelo con joblib\n",
    "    Parallel(n_jobs=-1)(delayed(compute_similarity_batch)(*batch) for batch in batches)\n",
    "\n",
    "    print(\"Procesos de cálculo de similitud completados.\")  # Mensaje de depuración\n",
    "\n",
    "    return temp_dir_d_drive\n",
    "\n",
    "# Calcular la matriz de similitud por lotes y guardar en el disco D:\\\n",
    "temp_dir = compute_similarity_parallel_joblib(embeddings_matrix_reduced)\n",
    "\n",
    "print(\"Cálculo de similitud completado y guardado en disco.\")  # Mensaje final de depuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DE ACA NO HE PODIDO LOGRAR HACER LA MATRIZ DE SIMILARIDAD, EL CODIGO ANTERIOR DESCARGAR TODAS LAS SIMILARIDADES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando tareas paralelas para cargar y ensamblar la matriz de similitud desde el disco...\n",
      "Total de lotes a procesar: 43365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 204 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 576 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1680 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2568 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done 3492 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4764 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5874 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6936 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8034 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9168 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 10805 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 12120 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 13283 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 15072 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 16884 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 18792 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 20661 tasks      | elapsed:  6.6min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Cargar la matriz de similitud desde el disco D:\\\u001b[39;00m\n\u001b[0;32m     49\u001b[0m n_tracks \u001b[38;5;241m=\u001b[39m embeddings_matrix_reduced\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 50\u001b[0m similarity_matrix_sparse \u001b[38;5;241m=\u001b[39m \u001b[43mload_similarity_matrix_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_dir_d_drive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tracks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatriz de similitud cargada desde el disco.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 33\u001b[0m, in \u001b[0;36mload_similarity_matrix_from_disk\u001b[1;34m(temp_dir, n_tracks, batch_size, n_jobs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal de lotes a procesar: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Depuración: número total de lotes\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Procesar lotes en paralelo\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_similarity_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTareas paralelas completadas. Ensamblando la matriz...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[1;32mc:\\Users\\chile\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chile\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chile\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy.sparse import lil_matrix, csr_matrix, save_npz, load_npz\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Definir la ubicación en el disco D:\\ para los archivos temporales\n",
    "temp_dir_d_drive = 'D:\\\\temp_similarity'\n",
    "\n",
    "def load_similarity_batch(file_path, i, end_i, j, end_j):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Cargando similitudes desde {file_path}\")\n",
    "        similarities = load_npz(file_path)\n",
    "        return (i, end_i, j, end_j, similarities)\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def load_similarity_matrix_from_disk(temp_dir, n_tracks, batch_size=500, n_jobs=-1):\n",
    "    similarity_matrix = lil_matrix((n_tracks, n_tracks), dtype=np.float32)\n",
    "    print(\"Iniciando tareas paralelas para cargar y ensamblar la matriz de similitud desde el disco...\")\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for i in range(0, n_tracks, batch_size):\n",
    "        end_i = min(i + batch_size, n_tracks)\n",
    "        for j in range(i, n_tracks, batch_size):\n",
    "            end_j = min(j + batch_size, n_tracks)\n",
    "            file_path = os.path.join(temp_dir, f'similarity_{i}_{j}.npz')\n",
    "            batches.append((file_path, i, end_i, j, end_j))\n",
    "\n",
    "    print(f\"Total de lotes a procesar: {len(batches)}\")  # Depuración: número total de lotes\n",
    "\n",
    "    # Procesar lotes en paralelo\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=5)(delayed(load_similarity_batch)(*batch) for batch in batches)\n",
    "\n",
    "    print(\"Tareas paralelas completadas. Ensamblando la matriz...\")\n",
    "\n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            i, end_i, j, end_j, similarities = result\n",
    "            similarity_matrix[i:end_i, j:end_j] = similarities\n",
    "            if i != j:\n",
    "                similarity_matrix[j:end_j, i:end_i] = similarities.transpose()\n",
    "            print(f\"Lotes {i}-{end_i} y {j}-{end_j} ensamblados.\")  # Depuración: ensamblaje de lotes\n",
    "\n",
    "    print(\"Matriz de similitud cargada y ensamblada desde el disco.\")\n",
    "    return similarity_matrix.tocsr()\n",
    "\n",
    "# Cargar la matriz de similitud desde el disco D:\\\n",
    "n_tracks = embeddings_matrix_reduced.shape[0]\n",
    "similarity_matrix_sparse = load_similarity_matrix_from_disk(temp_dir_d_drive, n_tracks)\n",
    "\n",
    "print(\"Matriz de similitud cargada desde el disco.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
